---
layout: post
title: "KDD Cup 2015"
date: 2015-12-07
---

Analytic competitions, such those hosted by Kaggle, provide opportunities to test and compare different algorithms for building predictive models.  A few months ago, I participated in the <a href="http://kddcup2015.com/information.html" a> 2015 KDD Cup </a>  with <a href="https://www.linkedin.com/in/jianjun-xie-a34a073" a> with a colleague </a> at CoreLogic.

The KDD Cup is a competition associated with the annual Knowledge Discovery and Data Mining conference.  The topic of the 2015 KDD Cup was predicting dropouts in Massive Open Online Courses (MOOC).  Dropout rates in MOOCs are very high, and <a href="http://wrap.warwick.ac.uk/65543/" a>numerous research papers </a>  have investigated <a href="http://www.editlib.org/p/147656/" a>reasons for this behavior</a>.  

Data were provided from XuetangX, a Chinese MOOC learning platform.  The outcome metric was course dropout during the next 10 days, given course and student information over time.  Student information included a record over time of participation in various aspects of the course (discussion forum, quiz etc).  A student ID was provided and could be used to link records for a given student across courses.  This could be used to calculate metrics such as student-level rates as model inputs.

The analytic team at CoreLogic had previously used the Kaggle <a href="https://www.kaggle.com/c/bike-sharing-demand" a> bike sharing demand competition </a> as a team-building exercise.   In that exercise, I had taken the approach of segmenting the data (by casual vs registered user, workday vs non workday, and season) and then estimating a negative binomial regression for each segment (using the glm.nb function in R).  My colleague used generalized boosted regression in his approach.  Blending our models to predict bike demand provided superior results than either of the individual approaches.

Given our favorable results in the bike sharing contest, we decided to take a similar approach in the KDD Cup 2015 contest.  My colleague once again used GBM to estimate his models.  My approach was to segment the data, and use logistic regression to estimate the dropout probability.  I segmented the data by course, which removed any course-level effects in the estimation process.  An additional important factor was the amount of times a student logged onto the course site.  A significant number of students logged onto the course only one time.  These students exhibited a significantly higher dropout rate than students who logged on to the course multiple times.  It was also possible to generate additional explanatory variables for students with multiple logons (e.g. time between logons). For this reason, I also segmented the data by multiple vs single logon.

I used the glmnet package in R to estimate a logistic regression with a penalty function.  The glmnet function can be used to estimate Lasso regression, ridge regression, as well as a combination of the two.  My intention in using glmnet was to use Lasso, or a function of Lasso, for variable reduction.

I've uploaded the R code for estimating glmnet <a href="https://github.com/SomeThingsConsidered/KDDCup2015/blob/master/glmnet_KDDCup2015.R" a> here. </a>  There are several points to highlight from the code: 

 <ul>
            <li>area under the curve (AUC) is used to evaluate the model performance.  This is consistent with the metric used to evaluate the model in the competition.</li>
            <li>K folds cross validation is used to evaluate the model performance.  K folds cross validation divides the data into K datasets, estimates the model on K-1 data, validates on the Kth data, repeats for each grouping, and takes the average of the validation results.</li>
            <li>a combination of lasso and ridge regression was used (lamda of 0.5) </li>
            <li>K folds cross validation can vary across draws of the data, especially for small datasets.  For this reason, the process was repeated 50 times, and the median result was selected </li>
 </ul>

Overall, this approach resulted in AUC metrics that were 0.01 - 0.02 below my colleague's approach of using GBM.  Given the AUC differences, combining the model approaches did not improve results relative to using GBM alone.

